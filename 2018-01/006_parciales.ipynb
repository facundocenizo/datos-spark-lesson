{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2016-01 Recuperatorio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una red social almacena el contenido de los chats entre sus usuarios\n",
    "en un RDD que tiene registros con el siguiente formato: (chat_id,\n",
    "user_id, nickname, text). Queremos saber cuál es el usuario (user_id)\n",
    "que mas preguntas hace en sus chats, contabilizamos una pregunta por\n",
    "cada caracter “?” que aparezca en el campo text. Programar en Spark\n",
    "un programa que identifique al usuario preguntón."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chats = [\n",
    "    (1, 1, 'damu', 'Qué es esto?'),\n",
    "    (2, 2, 'martin', 'Un chat!'),\n",
    "    (3, 1, 'damu', 'Ahhh! Y de donde salio? Whatsapp?'),\n",
    "    (4, 2, 'martin', 'Sí! Cómo sabias?'),\n",
    "    (5, 1, 'damu', 'Adivine'),\n",
    "    (6, 3, 'luis', 'Hola!')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = sc.parallelize(chats);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1, 'damu', 'Qu\\xc3\\xa9 es esto?'),\n",
       " (2, 2, 'martin', 'Un chat!'),\n",
       " (3, 1, 'damu', 'Ahhh! Y de donde salio? Whatsapp?'),\n",
       " (4, 2, 'martin', 'S\\xc3\\xad! C\\xc3\\xb3mo sabias?'),\n",
       " (5, 1, 'damu', 'Adivine'),\n",
       " (6, 3, 'luis', 'Hola!')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.map(lambda x: (x[1], x[3].count('?')))\\\n",
    "    .reduceByKey(lambda x, y: x+y)\\\n",
    "    .reduce(lambda x, y: x if (x[1] > y[1]) else y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2016-01 Parcial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UBER almacena en un cluster todos los datos sobre el movimiento y\n",
    "viajes de todos sus vehículos. Existe un proceso que nos devuelve un\n",
    "RDD llamado trip_summary con los siguientes campos: (driver_id,\n",
    "car_id, trip_id, customer_id, date (YYYYMMDD), distance_traveled),\n",
    "Programar usando Py Spark un programa que nos indique cual fue el\n",
    "conductor con mayor promedio de distancia recorrida por viaje para\n",
    "Abril de 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trips = [\n",
    "    (1, 1, 1, 1, '20160101', 10),\n",
    "    (2, 2, 2, 2, '20160202', 20),\n",
    "    (1, 1, 3, 1, '20160402', 15),\n",
    "    (1, 1, 4, 3, '20160405', 20),\n",
    "    (2, 2, 5, 4, '20160410', 25),\n",
    "    (3, 3, 6, 3, '20160415', 15),\n",
    "    (2, 2, 7, 1, '20160420', 40),\n",
    "    (3, 3, 8, 2, '20160505', 80)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = sc.parallelize(trips);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.filter(lambda x: (x[4] > '20160400') and (x[4] < '20160500'))\\\n",
    "    .map(lambda x: (x[0], (1, x[5])))\\\n",
    "    .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\\\n",
    "    .map(lambda x: (x[0], x[1][1]/x[1][0]))\\\n",
    "    .reduce(lambda x, y: x if x[1] > y[1] else y)\n",
    "    \n",
    "# El último map no es necesario, agregado solo por claridad. \n",
    "# Sin el map lo único que cambia es la comparación que se hace en el reduce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2015-02 2do Recuperatorio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un telescopio registra automaticamente la luminosidad de distintas\n",
    "estrellas generando un RDD con registros de tipo (star_id,\n",
    "magnitude,spectral_type, timestamp). Queremos obtener un listado de\n",
    "estrellas que tienen el mismo tipo espectral e igual promedio de\n",
    "magnitud una vez redondeado el mismo a un decimal. El resultado\n",
    "debe ser una lista en donde cada elemento de la lista es una lista de ids\n",
    "de estrellas similares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stars = [\n",
    "    (1, 5, 1, 1),\n",
    "    (2, 10, 1, 1),\n",
    "    (3, 6, 1, 1),\n",
    "    (4, 5.5, 2, 1),\n",
    "    (1, 6, 1, 2),\n",
    "    (2, 9, 1, 2),\n",
    "    (3, 5, 1, 2),\n",
    "    (1, 5.5, 1, 3),\n",
    "    (2, 11, 1, 3),\n",
    "    (3, 5.5, 1, 3)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = sc.parallelize(stars);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.map(lambda x: (x[0], (x[2], x[1], 1)))\\\n",
    "    .reduceByKey(lambda x, y: (x[0], x[1]+y[1], x[2]+y[2]))\\\n",
    "    .map(lambda x: ((x[1][0], x[1][1]/x[1][2]), x[0]))\\\n",
    "    .groupByKey()\\\n",
    "    .map(lambda x: (x[0], list(x[1])))\\\n",
    "    .collect()\n",
    "    \n",
    "# El último map es simplemente para convertir la sequencia de spark en una lista de python para poder verla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2015-01 2do Recuperatorio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos una colección de documentos (textos) almacenados en un cluster. Se quiere establecer un ranking de los patrones mas frecuentes para la aparición de letras en las palabras. Por ejemplo “sister”, “ejects” , “ninety” y “amazon” responden al patrón “abacde”. Programar en map-reduce un programa que genere como resultado un\n",
    "listado de tipo (patron, frecuencia) indicando cuántas veces aparece cada patrón en la colección de documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "textFile = sc.textFile(\"README.md\")\n",
    "words = textFile.flatMap(lambda line: line.split())\n",
    "words.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pattern(word):\n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    pat = ''\n",
    "    found = ''\n",
    "    for letter in word:\n",
    "        if letter in found:\n",
    "            pat += pat[found.index(letter)]\n",
    "        else:\n",
    "            found += letter\n",
    "            if len(found) > len(letters):\n",
    "                pat += '?'\n",
    "            else:\n",
    "                pat += letters[len(found)-1]\n",
    "    return pat\n",
    "   \n",
    "words.map(lambda x: (pattern(x), 1))\\\n",
    "     .reduceByKey(lambda x, y: x+y)\\\n",
    "     .takeOrdered(10, lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2016-02 Parcial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejercicio queremos programar un sistema que recomiende textos a usuarios en base a sus gustos sobre ciertos términos (palabras).\n",
    "\n",
    "Se cuenta con un RDD de textos de la forma (docId, texto) donde texto es un string de longitud variable.\n",
    "\n",
    "Además contamos con un RDD que indica qué términos le gustan o no a cada usuario de la forma (userId, término, score) por ejemplo (23, “calesita”, -2).\n",
    "\n",
    "Se pide programar en Spark un programa que calcule el score total de cada documento para cada usuario generando un RDD de la forma (userId, docId, score) en donde el score es simplemente la suma de los scores del usuario para los términos que aparecen en el documento.\n",
    "\n",
    "Puede haber términos en los documentos para los cuales no exista score de algunos usuarios, en estos casos simplemente los consideramos neutros (score=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = [\n",
    "    (1, 'pablo honey'),\n",
    "    (2, 'the bends'),\n",
    "    (3, 'ok computer'),\n",
    "    (4, 'kid a'),\n",
    "    (5, 'amnesiac'),\n",
    "    (6, 'hail to the thief'),\n",
    "    (7, 'in rainbows'),\n",
    "    (8, 'the king of limbs'),\n",
    "    (9, 'a moon shaped pool')\n",
    "]\n",
    "\n",
    "scores = [\n",
    "    ('thom', 'pablo', 1),\n",
    "    ('thom', 'honey', 1),\n",
    "    ('martin', 'pablo', -1),\n",
    "    ('martin', 'honey', -1),\n",
    "    ('martin', 'ok', 30),\n",
    "    ('martin', 'computer', 30)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documentsRDD = sc.parallelize(documents)\n",
    "scoresRDD = sc.parallelize(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pablo', 1),\n",
       " ('honey', 1),\n",
       " ('the', 2),\n",
       " ('bends', 2),\n",
       " ('ok', 3),\n",
       " ('computer', 3),\n",
       " ('kid', 4),\n",
       " ('a', 4),\n",
       " ('amnesiac', 5),\n",
       " ('hail', 6),\n",
       " ('to', 6),\n",
       " ('the', 6),\n",
       " ('thief', 6),\n",
       " ('in', 7),\n",
       " ('rainbows', 7),\n",
       " ('the', 8),\n",
       " ('king', 8),\n",
       " ('of', 8),\n",
       " ('limbs', 8),\n",
       " ('a', 9),\n",
       " ('moon', 9),\n",
       " ('shaped', 9),\n",
       " ('pool', 9)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generamos registros del tipo (word, docId) donde word sera nuestra key\n",
    "words = documentsRDD.flatMap(lambda a: [(word, a[0]) for word in a[1].split()])\n",
    "words.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pablo', ('thom', 1)),\n",
       " ('honey', ('thom', 1)),\n",
       " ('pablo', ('martin', -1)),\n",
       " ('honey', ('martin', -1)),\n",
       " ('ok', ('martin', 30)),\n",
       " ('computer', ('martin', 30))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# llevamos scores a una representacion donde word sea la clave\n",
    "scoresRDDForJoin = scoresRDD.map(lambda a: (a[1],(a[0],a[2])))\n",
    "scoresRDDForJoin.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('honey', (1, ('thom', 1))),\n",
       " ('honey', (1, ('martin', -1))),\n",
       " ('ok', (3, ('martin', 30))),\n",
       " ('pablo', (1, ('thom', 1))),\n",
       " ('pablo', (1, ('martin', -1))),\n",
       " ('computer', (3, ('martin', 30)))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_join_scores = words.join(scoresRDDForJoin)\n",
    "words_join_scores.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((1, 'thom'), 1),\n",
       " ((1, 'martin'), -1),\n",
       " ((3, 'martin'), 30),\n",
       " ((1, 'thom'), 1),\n",
       " ((1, 'martin'), -1),\n",
       " ((3, 'martin'), 30)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# para poder realizar una acumulacion debemos llevar la informacion a la representacion ((docId, userId), score) \n",
    "words_join_scores_mapped = words_join_scores.map(lambda a: ((a[1][0], a[1][1][0]), a[1][1][1]))\n",
    "words_join_scores_mapped.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 'martin', 60), (1, 'martin', -2), (1, 'thom', 2)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# acumulamos y llevamos a la representación pedida\n",
    "words_join_scores_mapped.reduceByKey(lambda a,b: a + b)\\\n",
    "                        .map(lambda a: (a[0][0], a[0][1], a[1]))\\\n",
    "                        .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2016-02 Recuperatorio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contamos con un cluster que tiene 4 computadoras.\n",
    "\n",
    "Queremos aprovechar el paralelismo del cluster para calcular los números primos entre 2 y 20.000.000. Para esto usaremos el conocido algoritmo de la criba de Eratóstenes. Por ejemplo si empezamos con una lista de tipo (2,3,4,5,6,7,8...) en un primer paso eliminamos todos los que son mayores a 2 y divisibles por 2 y nos queda (2,3,5,7,9,11,13…) luego eliminamos todos los mayores a 3 divisibles por 3 y nos queda (2,3,5,7,11,13….etc) luego todos los divisibles por 5 y así sucesivamente.\n",
    "\n",
    "El resultado final es una lista de números que son primos.\n",
    "\n",
    "Programar este programa usando PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Criterio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar hay que crear un RDD con todos los números y hacer un parallelize con 4 particiones. \n",
    "\n",
    "Luego la resolución pasa por un ciclo en el cual en cada paso del ciclo se obtenga como pivote el mínimo número del RDD que sea mayor al número usado como pivote anteriormente, luego simplemente hay que filtrar el RDD eliminando los números que son múltiplos del pivote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_multiple(pivot, value):\n",
    "        if (value == pivot):\n",
    "            return False\n",
    "    \n",
    "        if (value % pivot) == 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert is_multiple(3, 6)\n",
    "assert is_multiple(3, 9)\n",
    "assert not is_multiple(6, 6)\n",
    "assert not is_multiple(6, 3)\n",
    "assert not is_multiple(3, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# hacemos un caso de juguete de 2 a 2000\n",
    "numbers = sc.parallelize(range(2,2000), 4)\n",
    "\n",
    "pivot = 2\n",
    "\n",
    "while ((pivot*pivot) <= 2000):\n",
    "    new_pivot = numbers.filter(lambda a: a > pivot).reduce(lambda a, b: a if a < b else b)\n",
    "    numbers = numbers.filter(lambda a: not is_multiple(pivot, a)).cache()\n",
    "    pivot = new_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 3,\n",
       " 5,\n",
       " 7,\n",
       " 11,\n",
       " 13,\n",
       " 17,\n",
       " 19,\n",
       " 23,\n",
       " 29,\n",
       " 31,\n",
       " 37,\n",
       " 41,\n",
       " 43,\n",
       " 47,\n",
       " 53,\n",
       " 59,\n",
       " 61,\n",
       " 67,\n",
       " 71,\n",
       " 73,\n",
       " 79,\n",
       " 83,\n",
       " 89,\n",
       " 97,\n",
       " 101,\n",
       " 103,\n",
       " 107,\n",
       " 109,\n",
       " 113,\n",
       " 127,\n",
       " 131,\n",
       " 137,\n",
       " 139,\n",
       " 149,\n",
       " 151,\n",
       " 157,\n",
       " 163,\n",
       " 167,\n",
       " 173,\n",
       " 179,\n",
       " 181,\n",
       " 191,\n",
       " 193,\n",
       " 197,\n",
       " 199,\n",
       " 211,\n",
       " 223,\n",
       " 227,\n",
       " 229,\n",
       " 233,\n",
       " 239,\n",
       " 241,\n",
       " 251,\n",
       " 257,\n",
       " 263,\n",
       " 269,\n",
       " 271,\n",
       " 277,\n",
       " 281,\n",
       " 283,\n",
       " 293,\n",
       " 307,\n",
       " 311,\n",
       " 313,\n",
       " 317,\n",
       " 331,\n",
       " 337,\n",
       " 347,\n",
       " 349,\n",
       " 353,\n",
       " 359,\n",
       " 367,\n",
       " 373,\n",
       " 379,\n",
       " 383,\n",
       " 389,\n",
       " 397,\n",
       " 401,\n",
       " 409,\n",
       " 419,\n",
       " 421,\n",
       " 431,\n",
       " 433,\n",
       " 439,\n",
       " 443,\n",
       " 449,\n",
       " 457,\n",
       " 461,\n",
       " 463,\n",
       " 467,\n",
       " 479,\n",
       " 487,\n",
       " 491,\n",
       " 499,\n",
       " 503,\n",
       " 509,\n",
       " 521,\n",
       " 523,\n",
       " 541,\n",
       " 547,\n",
       " 557,\n",
       " 563,\n",
       " 569,\n",
       " 571,\n",
       " 577,\n",
       " 587,\n",
       " 593,\n",
       " 599,\n",
       " 601,\n",
       " 607,\n",
       " 613,\n",
       " 617,\n",
       " 619,\n",
       " 631,\n",
       " 641,\n",
       " 643,\n",
       " 647,\n",
       " 653,\n",
       " 659,\n",
       " 661,\n",
       " 673,\n",
       " 677,\n",
       " 683,\n",
       " 691,\n",
       " 701,\n",
       " 709,\n",
       " 719,\n",
       " 727,\n",
       " 733,\n",
       " 739,\n",
       " 743,\n",
       " 751,\n",
       " 757,\n",
       " 761,\n",
       " 769,\n",
       " 773,\n",
       " 787,\n",
       " 797,\n",
       " 809,\n",
       " 811,\n",
       " 821,\n",
       " 823,\n",
       " 827,\n",
       " 829,\n",
       " 839,\n",
       " 853,\n",
       " 857,\n",
       " 859,\n",
       " 863,\n",
       " 877,\n",
       " 881,\n",
       " 883,\n",
       " 887,\n",
       " 907,\n",
       " 911,\n",
       " 919,\n",
       " 929,\n",
       " 937,\n",
       " 941,\n",
       " 947,\n",
       " 953,\n",
       " 967,\n",
       " 971,\n",
       " 977,\n",
       " 983,\n",
       " 991,\n",
       " 997,\n",
       " 1009,\n",
       " 1013,\n",
       " 1019,\n",
       " 1021,\n",
       " 1031,\n",
       " 1033,\n",
       " 1039,\n",
       " 1049,\n",
       " 1051,\n",
       " 1061,\n",
       " 1063,\n",
       " 1069,\n",
       " 1087,\n",
       " 1091,\n",
       " 1093,\n",
       " 1097,\n",
       " 1103,\n",
       " 1109,\n",
       " 1117,\n",
       " 1123,\n",
       " 1129,\n",
       " 1151,\n",
       " 1153,\n",
       " 1163,\n",
       " 1171,\n",
       " 1181,\n",
       " 1187,\n",
       " 1193,\n",
       " 1201,\n",
       " 1213,\n",
       " 1217,\n",
       " 1223,\n",
       " 1229,\n",
       " 1231,\n",
       " 1237,\n",
       " 1249,\n",
       " 1259,\n",
       " 1277,\n",
       " 1279,\n",
       " 1283,\n",
       " 1289,\n",
       " 1291,\n",
       " 1297,\n",
       " 1301,\n",
       " 1303,\n",
       " 1307,\n",
       " 1319,\n",
       " 1321,\n",
       " 1327,\n",
       " 1361,\n",
       " 1367,\n",
       " 1373,\n",
       " 1381,\n",
       " 1399,\n",
       " 1409,\n",
       " 1423,\n",
       " 1427,\n",
       " 1429,\n",
       " 1433,\n",
       " 1439,\n",
       " 1447,\n",
       " 1451,\n",
       " 1453,\n",
       " 1459,\n",
       " 1471,\n",
       " 1481,\n",
       " 1483,\n",
       " 1487,\n",
       " 1489,\n",
       " 1493,\n",
       " 1499,\n",
       " 1511,\n",
       " 1523,\n",
       " 1531,\n",
       " 1543,\n",
       " 1549,\n",
       " 1553,\n",
       " 1559,\n",
       " 1567,\n",
       " 1571,\n",
       " 1579,\n",
       " 1583,\n",
       " 1597,\n",
       " 1601,\n",
       " 1607,\n",
       " 1609,\n",
       " 1613,\n",
       " 1619,\n",
       " 1621,\n",
       " 1627,\n",
       " 1637,\n",
       " 1657,\n",
       " 1663,\n",
       " 1667,\n",
       " 1669,\n",
       " 1693,\n",
       " 1697,\n",
       " 1699,\n",
       " 1709,\n",
       " 1721,\n",
       " 1723,\n",
       " 1733,\n",
       " 1741,\n",
       " 1747,\n",
       " 1753,\n",
       " 1759,\n",
       " 1777,\n",
       " 1783,\n",
       " 1787,\n",
       " 1789,\n",
       " 1801,\n",
       " 1811,\n",
       " 1823,\n",
       " 1831,\n",
       " 1847,\n",
       " 1861,\n",
       " 1867,\n",
       " 1871,\n",
       " 1873,\n",
       " 1877,\n",
       " 1879,\n",
       " 1889,\n",
       " 1901,\n",
       " 1907,\n",
       " 1913,\n",
       " 1931,\n",
       " 1933,\n",
       " 1949,\n",
       " 1951,\n",
       " 1973,\n",
       " 1979,\n",
       " 1987,\n",
       " 1993,\n",
       " 1997,\n",
       " 1999]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2017-01 Parcial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se tiene información estadística de la temporada regular de todos los jugadores de la NBA en un RDD de tuplas con el siguiente formato: (id_jugador, nombre, promedio_puntos, promedio_asistencias, promedio_robos, promedio_bloqueos, promedio_rebotes, promedio_faltas).\n",
    "\n",
    "Un analista de la cadena ESPN está trabajando con un RDD que corresponde a la primera ronda de playoffs y que tiene el siguiente formato: (id_jugador, id_partido, timestamp, cantidad_puntos, cantidad_rebotes, cantidad_bloqueos, cantidad_robos, cantidad_asistencias, cantidad_faltas).\n",
    "\n",
    "En base a estos RDDs se quiere programar en PySpark un programa que genere un RDD con los nombres (sin duplicados) de los jugadores que lograron en algún partido de playoffs una cantidad de asistencias mayor a su promedio histórico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Criterio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existen diversas maneras de realizarlo, pero es posible resolverlo realizando un reduceByKey con clave (id_jugador,id_partido) acumulando la cantidad de asistencias por partido quedando ((id_jugador, id_partido), sum_asistencias).\n",
    "\n",
    "Luego efectuamos un cambio de clave a id_jugador con un map para poder realizar un join de la informacion de la temporada usando id_jugador.\n",
    "\n",
    "Una vez hecho esto, filtramos aquella informacion que cumple de mayor cantidad asistencias en el partido que el promedio de la temporada.\n",
    "\n",
    "Dado que pueden quedarnos duplicados (un jugador podria superar su promedio de temporada en varios partidos), es importante al mapearla informacion al formato final realizar un distinct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# usamos para simplificar el formato, que puede obtenerse con un map.\n",
    "# (id_jugador, nombre, promedio_asistencias)\n",
    "players_all_time_stats = [\n",
    "    (1, 'Manu Ginobili', 800),\n",
    "    (2, 'Kobe Bryant', 100),\n",
    "    (3, 'Marc Gasol', 25),\n",
    "    (4, 'James Harden', 1000)]\n",
    "\n",
    "# usamos para simplificar el formato, que puede obtenerse con un map.\n",
    "# (id_jugador, id_partido, timestamp, cantidad_asistencias)\n",
    "scores = [\n",
    "  (1, 1, 1, 100),\n",
    "  (1, 1, 3, 100),\n",
    "  (2, 1, 1, 150),\n",
    "  (2, 1, 3, 150),\n",
    "  (3, 2, 2, 50),\n",
    "  (3, 2, 3, 50),      \n",
    "  (1, 2, 1, 150),\n",
    "  (1, 2, 3, 150),\n",
    "]\n",
    "\n",
    "players_all_time_stats_rdd = sc.parallelize(players_all_time_stats)\n",
    "scores_rdd = sc.parallelize(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((1, 1), 200), ((1, 2), 300), ((3, 2), 100), ((2, 1), 300)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cambio de clave a id_jugador, partido\n",
    "scores_by_match = scores_rdd.map(lambda a: ((a[0],a[1]),a[3]))\n",
    "scores_by_match = scores_by_match.reduceByKey(lambda a,b: a+b)\n",
    "scores_by_match.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 200), (1, 300), (3, 100), (2, 300)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cambio de clave a id_jugador\n",
    "scores_by_player = scores_by_match.map(lambda a: (a[0][0], a[1]))\n",
    "scores_by_player.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, (1, 'Manu Ginobili', 800)),\n",
       " (2, (2, 'Kobe Bryant', 100)),\n",
       " (3, (3, 'Marc Gasol', 25)),\n",
       " (4, (4, 'James Harden', 1000))]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# juntamos los datos para poder evaluar lo pedido\n",
    "# preparamos la key del lado de informacion historica de jugadores.\n",
    "\n",
    "all_time_with_key = players_all_time_stats_rdd.map(lambda a: (a[0],a))\n",
    "all_time_with_key.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, (200, (1, 'Manu Ginobili', 800))),\n",
       " (1, (300, (1, 'Manu Ginobili', 800))),\n",
       " (2, (300, (2, 'Kobe Bryant', 100))),\n",
       " (3, (100, (3, 'Marc Gasol', 25)))]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_by_player = scores_by_player.join(all_time_with_key)\n",
    "scores_by_player.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, (300, (2, 'Kobe Bryant', 100))), (3, (100, (3, 'Marc Gasol', 25)))]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overperformers = scores_by_player.filter(lambda a: a[1][0] > a[1][1][2])\n",
    "overperformers.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Kobe Bryant', 'Marc Gasol']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# veamos los duplicados\n",
    "overperformers = overperformers.map(lambda a: a[1][1][1]).distinct()\n",
    "overperformers.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
